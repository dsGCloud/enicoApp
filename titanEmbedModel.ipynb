{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pip install opensearch-py\n",
    "%pip install requests-aws4auth\n",
    "%pip install -U aiobotocore\n",
    "%pip install -U boto3\n",
    "%pip install -U botocore\n",
    "%pip install -U awscli\n",
    "%pip install -U s3fs\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import logging\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import sagemaker\n",
    "from utils import *\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "\n",
    "# getting boto3 clients for required AWS services\n",
    "sts_client = boto3.client('sts')\n",
    "s3_client = boto3.client('s3')\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    \"us-east-1\", \n",
    "    endpoint_url=\"https://bedrock-runtime.us-east-1.amazonaws.com\"\n",
    ")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region, account_id\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_role_arn = sagemaker.get_execution_role()\n",
    "sagemaker_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata\n",
    "\n",
    "You can use pandas to load metadata, then select products which have titles in US English from the data frame. You will use a column called main_image_id to merge item name with item image later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_json(\"s3://amazon-berkeley-objects/listings/metadata/listings_0.json.gz\", lines=True)\n",
    "def func_(x):\n",
    "    us_texts = [item[\"value\"] for item in x if item[\"language_tag\"] == \"en_US\"]\n",
    "    return us_texts[0] if us_texts else None\n",
    "\n",
    "meta = meta.assign(item_name_in_en_us=meta.item_name.apply(func_))\n",
    "meta = meta[~meta.item_name_in_en_us.isna()][[\"item_id\", \"item_name_in_en_us\", \"main_image_id\"]]\n",
    "print(f\"#products with US English title: {len(meta)}\")\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see over 1600 products in the data frame. Next, you can link the item names with item images. images/metadata/images.csv.gz contains Image metadata. This file is a gzip-compressed comma-separated value (CSV) file with the following columns: image_id, height, width, and path. You can read the meta data file and then merge it with item metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_meta = pd.read_csv(\"s3://amazon-berkeley-objects/images/metadata/images.csv.gz\")\n",
    "dataset = meta.merge(image_meta, left_on=\"main_image_id\", right_on=\"image_id\")\n",
    "# Create a new column in dataset with FULL PATH of the image\n",
    "dataset = dataset.assign(img_full_path=f's3://amazon-berkeley-objects/images/small/' + dataset.path.astype(str))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look at one sample image from the dataset by running the following code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image, item_name = get_image_from_item_id_s3(item_id = \"B0896LJNLH\", dataset = dataset, image_path = f's3://amazon-berkeley-objects/images/small/' )\n",
    "print(item_name)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate embedding from item images\n",
    "Amazon Titan Multimodal Embeddings G1 Generation 1 (G1) is able to project both images and text into the same latent space, so we only need to encode item images or texts into embedding space. In this practice, you can use batch inference to encode item images. Before creating the job, you need to copy item images from Amazon Berkeley Objects Dataset public S3 bucket to your own S3 Bucket. The operation needs take less than 10 mins.\n",
    "\n",
    "But for this notebook, we'll use real-time API than batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "multimodal_embeddings_img = []\n",
    "\n",
    "for idx, path in enumerate(dataset['img_full_path']):\n",
    "    embedding = get_titan_multimodal_embedding(image_path=path, dimension=1024)[\"embedding\"]\n",
    "    multimodal_embeddings_img.append(embedding)\n",
    "\n",
    "dataset = dataset.assign(embedding_img=multimodal_embeddings_img)\n",
    "dataset.head()\n",
    "# Store dataset\n",
    " dataset.to_csv('dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a vector store - OpenSearch Serverless index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "def create_policies_in_oss(vector_store_name, aoss_client, role_arn):\n",
    "    \n",
    "    encryption_policy_name = f\"titan-mm-sample-sp-{suffix}\"\n",
    "    network_policy_name = f\"titan-mm-sample-np-{suffix}\"\n",
    "    access_policy_name = f'titan-mm-sample-ap-{suffix}'\n",
    "\n",
    "    try:\n",
    "        encryption_policy = aoss_client.create_security_policy(\n",
    "            name=encryption_policy_name,\n",
    "            policy=json.dumps(\n",
    "                {\n",
    "                    'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                               'ResourceType': 'collection'}],\n",
    "                    'AWSOwnedKey': True\n",
    "                }),\n",
    "            type='encryption'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "    try:\n",
    "        network_policy = aoss_client.create_security_policy(\n",
    "            name=network_policy_name,\n",
    "            policy=json.dumps(\n",
    "                [\n",
    "                    {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                                'ResourceType': 'collection'}],\n",
    "                     'AllowFromPublic': True}\n",
    "                ]),\n",
    "            type='network'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        access_policy = aoss_client.create_access_policy(\n",
    "            name=access_policy_name,\n",
    "            policy=json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        'Rules': [\n",
    "                            {\n",
    "                                'Resource': ['collection/' + vector_store_name],\n",
    "                                'Permission': [\n",
    "                                    'aoss:CreateCollectionItems',\n",
    "                                    'aoss:DeleteCollectionItems',\n",
    "                                    'aoss:UpdateCollectionItems',\n",
    "                                    'aoss:DescribeCollectionItems'],\n",
    "                                'ResourceType': 'collection'\n",
    "                            },\n",
    "                            {\n",
    "                                'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                                'Permission': [\n",
    "                                    'aoss:CreateIndex',\n",
    "                                    'aoss:DeleteIndex',\n",
    "                                    'aoss:UpdateIndex',\n",
    "                                    'aoss:DescribeIndex',\n",
    "                                    'aoss:ReadDocument',\n",
    "                                    'aoss:WriteDocument'],\n",
    "                                'ResourceType': 'index'\n",
    "                            }],\n",
    "                        'Principal': [identity, role_arn],\n",
    "                        'Description': 'Easy data policy'}\n",
    "                ]),\n",
    "            type='data'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "    return encryption_policy, network_policy, access_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 create a new collection of type VECTORSEARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Collection\n",
    "vector_store_name = f'titan-mm-image-collection-{suffix}'\n",
    "\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       role_arn=sagemaker_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setting up the Amazon OpenSearch Serverless index using KNN settings\n",
    "Once the OpenSearch collection is created, create an index to store the item meta data and the embeddings. The index settings must be configured beforehand to enable the KNN functionality using the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"titam-mm-index\"\n",
    "index_body = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"image_vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1024 # Embedding size for Amanon Titan Multimodal Embedding G1 model, it is 1,024 (default), 384, 256\n",
    "         },\n",
    "         \"description\": {\"type\": \"text\"},\n",
    "          \"item_id\" : {\"type\": \"text\"},\n",
    "         \"image_url\": {\"type\": \"text\"}\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm its creation, we can retrieve the description of the new vector index you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would get an index already exists exception if the index already exists, and that is fine.\n",
    "try:\n",
    "    response = oss_client.indices.create(index_name, body=index_body)\n",
    "    print(f\"response received for the create index -> {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"error in creating index={index_name}, exception={e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%Optional\n",
    "### 4.3 Ingest the image embeddings\n",
    "Next you need to loop through your dataset and ingest items data into the cluster. A more robust and scalable solution for the embedding ingestion can be found in [Ingesting enriched data into Amazon ES](https://aws.amazon.com/blogs/industries/novartis-ag-uses-amazon-elasticsearch-k-nearest-neighbor-knn-and-amazon-sagemaker-to-power-search-and-recommendation/). The data ingestion for this POC should finish within 60 seconds. It also executes a simple query to verify the data have been ingested into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "for idx, record in tq.tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "    document = {\n",
    "                'image_vector': dataset['embedding_img'][idx],\n",
    "                \"description\":   dataset['item_name_in_en_us'][idx],\n",
    "                \"item_id\" : dataset['item_id'][idx],\n",
    "                \"image_url\": dataset['img_full_path'][idx],  \n",
    "                }\n",
    "    response = oss_client.index(\n",
    "    index = index_name,\n",
    "    body = document\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Perform a real-time Multimodal Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"articulos de limpieza para mascota\"\n",
    "similar_items = find_similar_items_from_query(query_prompt = query_prompt, k=2, num_results=10, index_name=index_name, dataset = dataset, \n",
    "                                   open_search_client = oss_client, image_root_path=f's3://amazon-berkeley-objects/images/small/')\n",
    "\n",
    "display_images(similar_items)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
